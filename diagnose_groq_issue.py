#!/usr/bin/env python3
"""
Diagnostic script to identify GROQ integration issues
"""
import asyncio
import os
from config import settings

def check_environment():
    """Check environment configuration"""
    print("ğŸ” ENVIRONMENT DIAGNOSTICS")
    print("=" * 40)
    
    print(f"ğŸ”§ LLM_PROVIDER: {settings.llm_provider}")
    print(f"ğŸ”‘ GROQ_API_KEY: {'âœ… Set' if settings.groq_api_key else 'âŒ Missing'}")
    print(f"ğŸ¤– GROQ_MODEL: {settings.groq_model}")
    
    # Check .env file
    env_file = ".env"
    if os.path.exists(env_file):
        print(f"ğŸ“„ .env file: âœ… Found")
        with open(env_file, 'r') as f:
            content = f.read()
            if "LLM_PROVIDER=groq" in content:
                print("âœ… LLM_PROVIDER=groq found in .env")
            else:
                print("âš ï¸  LLM_PROVIDER=groq not found in .env")
    else:
        print(f"ğŸ“„ .env file: âŒ Not found")
    
    return settings.llm_provider == "groq" and settings.groq_api_key

async def test_llm_client_direct():
    """Test LLM client directly"""
    print("\nğŸ§ª TESTING LLM CLIENT DIRECTLY")
    print("=" * 40)
    
    try:
        from llm_client import get_llm_client
        
        # Force GROQ provider
        original_provider = settings.llm_provider
        settings.llm_provider = "groq"
        
        print("ğŸš€ Initializing LLM client...")
        client = get_llm_client()
        print("âœ… LLM client initialized")
        
        print("ğŸ’¬ Testing chat completion...")
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Say 'Hello, GROQ is working!' in exactly those words."}
        ]
        
        response = await client.chat_completion(messages, max_tokens=50)
        print(f"âœ… Chat completion successful!")
        print(f"ğŸ“ Response: {response}")
        
        # Restore original provider
        settings.llm_provider = original_provider
        return True
        
    except Exception as e:
        print(f"âŒ LLM client test failed: {str(e)}")
        settings.llm_provider = original_provider
        return False

async def test_orchestrator():
    """Test the orchestrator's LLM integration"""
    print("\nğŸ¯ TESTING ORCHESTRATOR LLM INTEGRATION")
    print("=" * 40)
    
    try:
        # Import the working orchestrator
        from main_working import WorkingOrchestrator
        from models import QueryRequest
        
        print("ğŸš€ Initializing orchestrator...")
        orchestrator = WorkingOrchestrator()
        
        if orchestrator.llm_client:
            print("âœ… Orchestrator has LLM client")
        else:
            print("âŒ Orchestrator missing LLM client")
            return False
        
        print("ğŸ’¬ Testing query processing...")
        request = QueryRequest(prompt="Hello, can you help me find files?", max_results=5)
        result = await orchestrator.process_query(request)
        
        print(f"âœ… Query processed successfully!")
        print(f"ğŸ“Š Results: {len(result.results)} agent results")
        
        if result.answer:
            print(f"ğŸ¤– LLM Answer: {result.answer[:100]}...")
            return True
        else:
            print("âš ï¸  No LLM answer generated")
            return False
            
    except Exception as e:
        print(f"âŒ Orchestrator test failed: {str(e)}")
        return False

async def test_backend_integration():
    """Test full backend integration"""
    print("\nğŸŒ TESTING BACKEND INTEGRATION")
    print("=" * 40)
    
    try:
        import requests
        
        print("ğŸ” Testing search endpoint...")
        response = requests.post(
            "http://localhost:8001/search",
            json={"prompt": "Find Python files in this project"},
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            print("âœ… Backend request successful!")
            
            if "answer" in data and data["answer"]:
                print(f"ğŸ¤– Backend LLM Answer: {data['answer'][:100]}...")
                return True
            else:
                print("âš ï¸  Backend returned no LLM answer")
                print(f"ğŸ“Š Response keys: {list(data.keys())}")
                return False
        else:
            print(f"âŒ Backend request failed: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Backend test failed: {str(e)}")
        return False

async def main():
    """Main diagnostic function"""
    print("ğŸ”§ GROQ INTEGRATION DIAGNOSTICS")
    print("=" * 50)
    
    # Step 1: Check environment
    env_ok = check_environment()
    
    if not env_ok:
        print("\nâŒ ENVIRONMENT ISSUES DETECTED")
        print("ğŸ”§ Fix these issues first:")
        print("1. Create a .env file in the project root")
        print("2. Add: LLM_PROVIDER=groq")
        print("3. Add: GROQ_API_KEY=your_actual_key")
        print("4. Add: GROQ_MODEL=llama3-8b-8192")
        return
    
    # Step 2: Test LLM client
    client_ok = await test_llm_client_direct()
    
    # Step 3: Test orchestrator
    orchestrator_ok = await test_orchestrator()
    
    # Step 4: Test backend
    backend_ok = await test_backend_integration()
    
    print("\n" + "=" * 50)
    print("ğŸ“Š DIAGNOSTIC RESULTS")
    print("=" * 50)
    
    print(f"ğŸ”§ Environment: {'âœ…' if env_ok else 'âŒ'}")
    print(f"ğŸ¤– LLM Client: {'âœ…' if client_ok else 'âŒ'}")
    print(f"ğŸ¯ Orchestrator: {'âœ…' if orchestrator_ok else 'âŒ'}")
    print(f"ğŸŒ Backend: {'âœ…' if backend_ok else 'âŒ'}")
    
    if all([env_ok, client_ok, orchestrator_ok, backend_ok]):
        print("\nğŸ‰ ALL TESTS PASSED!")
        print("âœ… GROQ is working correctly with your system")
    else:
        print("\nâš ï¸  ISSUES DETECTED")
        print("ğŸ”§ Follow the specific error messages above to fix the issues")

if __name__ == "__main__":
    asyncio.run(main())
